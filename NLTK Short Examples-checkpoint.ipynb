{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abdb3895",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc9bca8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The goal of this course is to provide advanced training in data science, encompassing current theory and application, to real-world data analytics problems. This course builds on the fundamentals established in CSCI 3360 Data Science I, exploring advanced topics such as nonlinear dimensionality reduction, semi-supervised learning, graph algorithms, and deep learning.\n"
     ]
    }
   ],
   "source": [
    "# A \n",
    "testString =  \"The goal of this course is to provide advanced training \" + \\\n",
    "                \"in data science, encompassing current theory and \" + \\\n",
    "                \"application, to real-world data analytics problems. \" + \\\n",
    "                \"This course builds on the fundamentals established \" + \\\n",
    "                \"in CSCI 3360 Data Science I, exploring advanced topics \" + \\\n",
    "                \"such as nonlinear dimensionality reduction, semi-supervised \" + \\\n",
    "                \"learning, graph algorithms, and deep learning.\"\n",
    "print(testString)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a829d4",
   "metadata": {},
   "source": [
    "### Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f65d4f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T\n",
      "h\n",
      "e\n",
      " \n",
      "g\n",
      "o\n",
      "a\n",
      "l\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "t\n",
      "h\n",
      "i\n",
      "s\n",
      " \n",
      "c\n",
      "o\n",
      "u\n",
      "r\n",
      "s\n",
      "e\n",
      " \n",
      "i\n",
      "s\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "p\n",
      "r\n",
      "o\n",
      "v\n",
      "i\n",
      "d\n",
      "e\n",
      " \n",
      "a\n",
      "d\n",
      "v\n",
      "a\n",
      "n\n",
      "c\n",
      "e\n",
      "d\n",
      " \n",
      "t\n",
      "r\n",
      "a\n",
      "i\n",
      "n\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "d\n",
      "a\n",
      "t\n",
      "a\n",
      " \n",
      "s\n",
      "c\n",
      "i\n",
      "e\n",
      "n\n",
      "c\n",
      "e\n",
      ",\n",
      " \n",
      "e\n",
      "n\n",
      "c\n",
      "o\n",
      "m\n",
      "p\n",
      "a\n",
      "s\n",
      "s\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "c\n",
      "u\n",
      "r\n",
      "r\n",
      "e\n",
      "n\n",
      "t\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      "o\n",
      "r\n",
      "y\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "a\n",
      "p\n",
      "p\n",
      "l\n",
      "i\n",
      "c\n",
      "a\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      ",\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "r\n",
      "e\n",
      "a\n",
      "l\n",
      "-\n",
      "w\n",
      "o\n",
      "r\n",
      "l\n",
      "d\n",
      " \n",
      "d\n",
      "a\n",
      "t\n",
      "a\n",
      " \n",
      "a\n",
      "n\n",
      "a\n",
      "l\n",
      "y\n",
      "t\n",
      "i\n",
      "c\n",
      "s\n",
      " \n",
      "p\n",
      "r\n",
      "o\n",
      "b\n",
      "l\n",
      "e\n",
      "m\n",
      "s\n",
      ".\n",
      " \n",
      "T\n",
      "h\n",
      "i\n",
      "s\n",
      " \n",
      "c\n",
      "o\n",
      "u\n",
      "r\n",
      "s\n",
      "e\n",
      " \n",
      "b\n",
      "u\n",
      "i\n",
      "l\n",
      "d\n",
      "s\n",
      " \n",
      "o\n",
      "n\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "f\n",
      "u\n",
      "n\n",
      "d\n",
      "a\n",
      "m\n",
      "e\n",
      "n\n",
      "t\n",
      "a\n",
      "l\n",
      "s\n",
      " \n",
      "e\n",
      "s\n",
      "t\n",
      "a\n",
      "b\n",
      "l\n",
      "i\n",
      "s\n",
      "h\n",
      "e\n",
      "d\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "C\n",
      "S\n",
      "C\n",
      "I\n",
      " \n",
      "3\n",
      "3\n",
      "6\n",
      "0\n",
      " \n",
      "D\n",
      "a\n",
      "t\n",
      "a\n",
      " \n",
      "S\n",
      "c\n",
      "i\n",
      "e\n",
      "n\n",
      "c\n",
      "e\n",
      " \n",
      "I\n",
      ",\n",
      " \n",
      "e\n",
      "x\n",
      "p\n",
      "l\n",
      "o\n",
      "r\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "a\n",
      "d\n",
      "v\n",
      "a\n",
      "n\n",
      "c\n",
      "e\n",
      "d\n",
      " \n",
      "t\n",
      "o\n",
      "p\n",
      "i\n",
      "c\n",
      "s\n",
      " \n",
      "s\n",
      "u\n",
      "c\n",
      "h\n",
      " \n",
      "a\n",
      "s\n",
      " \n",
      "n\n",
      "o\n",
      "n\n",
      "l\n",
      "i\n",
      "n\n",
      "e\n",
      "a\n",
      "r\n",
      " \n",
      "d\n",
      "i\n",
      "m\n",
      "e\n",
      "n\n",
      "s\n",
      "i\n",
      "o\n",
      "n\n",
      "a\n",
      "l\n",
      "i\n",
      "t\n",
      "y\n",
      " \n",
      "r\n",
      "e\n",
      "d\n",
      "u\n",
      "c\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      ",\n",
      " \n",
      "s\n",
      "e\n",
      "m\n",
      "i\n",
      "-\n",
      "s\n",
      "u\n",
      "p\n",
      "e\n",
      "r\n",
      "v\n",
      "i\n",
      "s\n",
      "e\n",
      "d\n",
      " \n",
      "l\n",
      "e\n",
      "a\n",
      "r\n",
      "n\n",
      "i\n",
      "n\n",
      "g\n",
      ",\n",
      " \n",
      "g\n",
      "r\n",
      "a\n",
      "p\n",
      "h\n",
      " \n",
      "a\n",
      "l\n",
      "g\n",
      "o\n",
      "r\n",
      "i\n",
      "t\n",
      "h\n",
      "m\n",
      "s\n",
      ",\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "d\n",
      "e\n",
      "e\n",
      "p\n",
      " \n",
      "l\n",
      "e\n",
      "a\n",
      "r\n",
      "n\n",
      "i\n",
      "n\n",
      "g\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# NLTK has a quick method for creating a list of words in a string when normal list comprehensions are not effective\n",
    "for i in testString:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5787c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      "goal\n",
      "of\n",
      "this\n",
      "course\n",
      "is\n",
      "to\n",
      "provide\n",
      "advanced\n",
      "training\n",
      "in\n",
      "data\n",
      "science\n",
      ",\n",
      "encompassing\n",
      "current\n",
      "theory\n",
      "and\n",
      "application\n",
      ",\n",
      "to\n",
      "real-world\n",
      "data\n",
      "analytics\n",
      "problems\n",
      ".\n",
      "This\n",
      "course\n",
      "builds\n",
      "on\n",
      "the\n",
      "fundamentals\n",
      "established\n",
      "in\n",
      "CSCI\n",
      "3360\n",
      "Data\n",
      "Science\n",
      "I\n",
      ",\n",
      "exploring\n",
      "advanced\n",
      "topics\n",
      "such\n",
      "as\n",
      "nonlinear\n",
      "dimensionality\n",
      "reduction\n",
      ",\n",
      "semi-supervised\n",
      "learning\n",
      ",\n",
      "graph\n",
      "algorithms\n",
      ",\n",
      "and\n",
      "deep\n",
      "learning\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "words = word_tokenize(testString)\n",
    "for word in words:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1bf9ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e515a70",
   "metadata": {},
   "source": [
    "### Basic Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee56c0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# There is an corpus of stopwords included in the NLTK library\n",
    "from nltk.corpus import stopwords\n",
    "stpWords = stopwords.words('english')\n",
    "print(stpWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "841a2c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a quick list to string method to view results\n",
    "def toString(words):\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37019aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant Words:  The goal course provide advanced training data science , encompassing current theory application , real-world data analytics problems . This course builds fundamentals established CSCI 3360 Data Science I , exploring advanced topics nonlinear dimensionality reduction , semi-supervised learning , graph algorithms , deep learning .\n",
      "\n",
      "Irrelevant Words:  of this is to in and to on the in such as and\n"
     ]
    }
   ],
   "source": [
    "# We can use a simple list comprehension to remove any instances of stopwords from our list of tokens\n",
    "relevantWords = []\n",
    "removedWords = []\n",
    "for word in words:\n",
    "    if word in stpWords:\n",
    "        removedWords.append(word)\n",
    "    if word not in stpWords:\n",
    "        relevantWords.append(word)\n",
    "print(\"Relevant Words: \", toString(relevantWords))\n",
    "print()\n",
    "print(\"Irrelevant Words: \", toString(removedWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ccfcb843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can define a method with this technique for faster testing\n",
    "def removeStopWords(words, stop_words):\n",
    "    cleaned = []\n",
    "    for word in words:\n",
    "        if word.lower() not in stop_words:\n",
    "            cleaned.append(word.lower()) # Lowercase output\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e7439d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The goal of this course is to provide advanced training in data science encompassing current theory and application to real world data analytics problems This course builds on the fundamentals established in CSCI 3360 Data Science I exploring advanced topics such as nonlinear dimensionality reduction semi supervised learning graph algorithms and deep learning\n"
     ]
    }
   ],
   "source": [
    "# There is still some unnecessary punctuation we can remove\n",
    "# We can use the regex tokenizer for more slightly more advanced processing\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\") #Idk what this token is I should probably figure it out\n",
    "words = tokenizer.tokenize(testString)\n",
    "print(toString(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "be533a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goal course provide advanced training data science encompassing current theory application real world data analytics problems course builds fundamentals established csci 3360 data science exploring advanced topics nonlinear dimensionality reduction semi supervised learning graph algorithms deep learning\n"
     ]
    }
   ],
   "source": [
    "# We can use the method defined earlier to clean further\n",
    "words = removeStopWords(words, stpWords)\n",
    "print(toString(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d20d6804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goal cours provid advanc train data scienc encompass current theori applic real world data analyt problem cours build fundament establish csci 3360 data scienc explor advanc topic nonlinear dimension reduct semi supervis learn graph algorithm deep learn\n"
     ]
    }
   ],
   "source": [
    "# In order to reduce variation in words we can make use of a simplistic stemming technique\n",
    "# The porter stemmer is common\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "wordsStemmed = [stemmer.stem(word) for word in words]\n",
    "print(toString(wordsStemmed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8762637c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
