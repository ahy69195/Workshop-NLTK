{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8823fc0",
   "metadata": {},
   "source": [
    "# NLTK Basics\n",
    "### Gettting Started with NLTK\n",
    "!pip install nltk\n",
    "\n",
    "nltk.download()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abdb3895",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc9bca8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The goal of this course is to provide advanced training in data science, encompassing current theory and application, to real-world data analytics problems. This course builds on the fundamentals established in CSCI 3360 Data Science I, exploring advanced topics such as nonlinear dimensionality reduction, semi-supervised learning, graph algorithms, and deep learning.\n"
     ]
    }
   ],
   "source": [
    "# A short string example for use in text processing\n",
    "# This is the course description on the Data Science II website\n",
    "testString =  \"The goal of this course is to provide advanced training \" + \\\n",
    "                \"in data science, encompassing current theory and \" + \\\n",
    "                \"application, to real-world data analytics problems. \" + \\\n",
    "                \"This course builds on the fundamentals established \" + \\\n",
    "                \"in CSCI 3360 Data Science I, exploring advanced topics \" + \\\n",
    "                \"such as nonlinear dimensionality reduction, semi-supervised \" + \\\n",
    "                \"learning, graph algorithms, and deep learning.\"\n",
    "print(testString)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a829d4",
   "metadata": {},
   "source": [
    "### Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f65d4f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T\n",
      "h\n",
      "e\n",
      " \n",
      "g\n",
      "o\n",
      "a\n",
      "l\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "t\n",
      "h\n",
      "i\n",
      "s\n",
      " \n",
      "c\n",
      "o\n",
      "u\n",
      "r\n",
      "s\n",
      "e\n",
      " \n",
      "i\n",
      "s\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "p\n",
      "r\n",
      "o\n",
      "v\n",
      "i\n",
      "d\n",
      "e\n",
      " \n",
      "a\n",
      "d\n",
      "v\n",
      "a\n",
      "n\n",
      "c\n",
      "e\n",
      "d\n",
      " \n",
      "t\n",
      "r\n",
      "a\n",
      "i\n",
      "n\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "d\n",
      "a\n",
      "t\n",
      "a\n",
      " \n",
      "s\n",
      "c\n",
      "i\n",
      "e\n",
      "n\n",
      "c\n",
      "e\n",
      ",\n",
      " \n",
      "e\n",
      "n\n",
      "c\n",
      "o\n",
      "m\n",
      "p\n",
      "a\n",
      "s\n",
      "s\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "c\n",
      "u\n",
      "r\n",
      "r\n",
      "e\n",
      "n\n",
      "t\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      "o\n",
      "r\n",
      "y\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "a\n",
      "p\n",
      "p\n",
      "l\n",
      "i\n",
      "c\n",
      "a\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      ",\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "r\n",
      "e\n",
      "a\n",
      "l\n",
      "-\n",
      "w\n",
      "o\n",
      "r\n",
      "l\n",
      "d\n",
      " \n",
      "d\n",
      "a\n",
      "t\n",
      "a\n",
      " \n",
      "a\n",
      "n\n",
      "a\n",
      "l\n",
      "y\n",
      "t\n",
      "i\n",
      "c\n",
      "s\n",
      " \n",
      "p\n",
      "r\n",
      "o\n",
      "b\n",
      "l\n",
      "e\n",
      "m\n",
      "s\n",
      ".\n",
      " \n",
      "T\n",
      "h\n",
      "i\n",
      "s\n",
      " \n",
      "c\n",
      "o\n",
      "u\n",
      "r\n",
      "s\n",
      "e\n",
      " \n",
      "b\n",
      "u\n",
      "i\n",
      "l\n",
      "d\n",
      "s\n",
      " \n",
      "o\n",
      "n\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "f\n",
      "u\n",
      "n\n",
      "d\n",
      "a\n",
      "m\n",
      "e\n",
      "n\n",
      "t\n",
      "a\n",
      "l\n",
      "s\n",
      " \n",
      "e\n",
      "s\n",
      "t\n",
      "a\n",
      "b\n",
      "l\n",
      "i\n",
      "s\n",
      "h\n",
      "e\n",
      "d\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "C\n",
      "S\n",
      "C\n",
      "I\n",
      " \n",
      "3\n",
      "3\n",
      "6\n",
      "0\n",
      " \n",
      "D\n",
      "a\n",
      "t\n",
      "a\n",
      " \n",
      "S\n",
      "c\n",
      "i\n",
      "e\n",
      "n\n",
      "c\n",
      "e\n",
      " \n",
      "I\n",
      ",\n",
      " \n",
      "e\n",
      "x\n",
      "p\n",
      "l\n",
      "o\n",
      "r\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "a\n",
      "d\n",
      "v\n",
      "a\n",
      "n\n",
      "c\n",
      "e\n",
      "d\n",
      " \n",
      "t\n",
      "o\n",
      "p\n",
      "i\n",
      "c\n",
      "s\n",
      " \n",
      "s\n",
      "u\n",
      "c\n",
      "h\n",
      " \n",
      "a\n",
      "s\n",
      " \n",
      "n\n",
      "o\n",
      "n\n",
      "l\n",
      "i\n",
      "n\n",
      "e\n",
      "a\n",
      "r\n",
      " \n",
      "d\n",
      "i\n",
      "m\n",
      "e\n",
      "n\n",
      "s\n",
      "i\n",
      "o\n",
      "n\n",
      "a\n",
      "l\n",
      "i\n",
      "t\n",
      "y\n",
      " \n",
      "r\n",
      "e\n",
      "d\n",
      "u\n",
      "c\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      ",\n",
      " \n",
      "s\n",
      "e\n",
      "m\n",
      "i\n",
      "-\n",
      "s\n",
      "u\n",
      "p\n",
      "e\n",
      "r\n",
      "v\n",
      "i\n",
      "s\n",
      "e\n",
      "d\n",
      " \n",
      "l\n",
      "e\n",
      "a\n",
      "r\n",
      "n\n",
      "i\n",
      "n\n",
      "g\n",
      ",\n",
      " \n",
      "g\n",
      "r\n",
      "a\n",
      "p\n",
      "h\n",
      " \n",
      "a\n",
      "l\n",
      "g\n",
      "o\n",
      "r\n",
      "i\n",
      "t\n",
      "h\n",
      "m\n",
      "s\n",
      ",\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "d\n",
      "e\n",
      "e\n",
      "p\n",
      " \n",
      "l\n",
      "e\n",
      "a\n",
      "r\n",
      "n\n",
      "i\n",
      "n\n",
      "g\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# NLTK has a quick method for creating a list of words in a string when normal list comprehensions are not effective\n",
    "for i in testString:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5787c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      "goal\n",
      "of\n",
      "this\n",
      "course\n",
      "is\n",
      "to\n",
      "provide\n",
      "advanced\n",
      "training\n",
      "in\n",
      "data\n",
      "science\n",
      ",\n",
      "encompassing\n",
      "current\n",
      "theory\n",
      "and\n",
      "application\n",
      ",\n",
      "to\n",
      "real-world\n",
      "data\n",
      "analytics\n",
      "problems\n",
      ".\n",
      "This\n",
      "course\n",
      "builds\n",
      "on\n",
      "the\n",
      "fundamentals\n",
      "established\n",
      "in\n",
      "CSCI\n",
      "3360\n",
      "Data\n",
      "Science\n",
      "I\n",
      ",\n",
      "exploring\n",
      "advanced\n",
      "topics\n",
      "such\n",
      "as\n",
      "nonlinear\n",
      "dimensionality\n",
      "reduction\n",
      ",\n",
      "semi-supervised\n",
      "learning\n",
      ",\n",
      "graph\n",
      "algorithms\n",
      ",\n",
      "and\n",
      "deep\n",
      "learning\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# The NLTK word tokenizer converts strings to lists\n",
    "# It works for more languages than english\n",
    "from nltk.tokenize import word_tokenize\n",
    "words = word_tokenize(testString, language='english')\n",
    "for word in words:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1bf9ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# Returns a list object\n",
    "print(type(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e515a70",
   "metadata": {},
   "source": [
    "### Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee56c0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# There is an corpus of stopwords included in the NLTK library\n",
    "# These words represent little in the context of the text\n",
    "from nltk.corpus import stopwords\n",
    "stpWords = stopwords.words('english')\n",
    "print(stpWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "841a2c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a quick list to string method to view results\n",
    "def toString(words):\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37019aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant Words:  goal course provide advanced training data science , encompassing current theory application , real-world data analytics problems . course builds fundamentals established csci 3360 data science , exploring advanced topics nonlinear dimensionality reduction , semi-supervised learning , graph algorithms , deep learning .\n",
      "\n",
      "Irrelevant Words:  the of this is to in and to this on the in i such as and\n"
     ]
    }
   ],
   "source": [
    "# We can use a simple list comprehension to remove any instances of stopwords from our list of tokens\n",
    "relevantWords = []\n",
    "removedWords = []\n",
    "for word in words:\n",
    "    if word.lower() in stpWords:\n",
    "        removedWords.append(word.lower())\n",
    "    if word.lower() not in stpWords:\n",
    "        relevantWords.append(word.lower())\n",
    "print(\"Relevant Words: \", toString(relevantWords))\n",
    "print()\n",
    "print(\"Irrelevant Words: \", toString(removedWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccfcb843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can define a method with this technique for faster testing\n",
    "def removeStopWords(words, stop_words):\n",
    "    cleaned = []\n",
    "    for word in words:\n",
    "        if word.lower() not in stop_words:\n",
    "            cleaned.append(word.lower()) # Lowercase output\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2bafa9",
   "metadata": {},
   "source": [
    "### Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7439d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The goal of this course is to provide advanced training in data science encompassing current theory and application to real world data analytics problems This course builds on the fundamentals established in CSCI 3360 Data Science I exploring advanced topics such as nonlinear dimensionality reduction semi supervised learning graph algorithms and deep learning\n"
     ]
    }
   ],
   "source": [
    "# There is still some unnecessary punctuation we can remove\n",
    "# We can use the regex tokenizer for more slightly more advanced processing\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\") # r-string literal\n",
    "words = tokenizer.tokenize(testString)\n",
    "print(toString(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be533a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goal course provide advanced training data science encompassing current theory application real world data analytics problems course builds fundamentals established csci 3360 data science exploring advanced topics nonlinear dimensionality reduction semi supervised learning graph algorithms deep learning\n"
     ]
    }
   ],
   "source": [
    "# We can use the method defined earlier to clean stop words once again\n",
    "wordsRelevant = removeStopWords(words, stpWords)\n",
    "print(toString(wordsRelevant))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cd5b69",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "572bb090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goal cours provid advanc train data scienc encompass current theori applic real world data analyt problem cours build fundament establish csci 3360 data scienc explor advanc topic nonlinear dimension reduct semi supervis learn graph algorithm deep learn\n"
     ]
    }
   ],
   "source": [
    "# In order to reduce variation in words we can make use of a simplistic stemming technique\n",
    "# The porter stemmer is common\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "wordsStemmed = [stemmer.stem(word) for word in wordsRelevant]\n",
    "print(toString(wordsStemmed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b784ef",
   "metadata": {},
   "source": [
    "### Part-of-Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4c3b1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DET'), ('goal', 'NOUN'), ('of', 'ADP'), ('this', 'DET'), ('course', 'NOUN'), ('is', 'VERB'), ('to', 'PRT'), ('provide', 'VERB'), ('advanced', 'ADJ'), ('training', 'NOUN'), ('in', 'ADP'), ('data', 'NOUN'), ('science', 'NOUN'), ('encompassing', 'VERB'), ('current', 'ADJ'), ('theory', 'NOUN'), ('and', 'CONJ'), ('application', 'NOUN'), ('to', 'PRT'), ('real', 'ADJ'), ('world', 'NOUN'), ('data', 'NOUN'), ('analytics', 'NOUN'), ('problems', 'NOUN'), ('This', 'DET'), ('course', 'NOUN'), ('builds', 'VERB'), ('on', 'ADP'), ('the', 'DET'), ('fundamentals', 'NOUN'), ('established', 'VERB'), ('in', 'ADP'), ('CSCI', 'NOUN'), ('3360', 'NUM'), ('Data', 'NOUN'), ('Science', 'NOUN'), ('I', 'PRON'), ('exploring', 'VERB'), ('advanced', 'ADJ'), ('topics', 'NOUN'), ('such', 'ADJ'), ('as', 'ADP'), ('nonlinear', 'ADJ'), ('dimensionality', 'NOUN'), ('reduction', 'NOUN'), ('semi', 'NOUN'), ('supervised', 'VERB'), ('learning', 'VERB'), ('graph', 'NOUN'), ('algorithms', 'NOUN'), ('and', 'CONJ'), ('deep', 'ADJ'), ('learning', 'NOUN')]\n",
      "\n",
      "Output type:  <class 'list'> <class 'tuple'>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can get a better representation of the distilled words using NLTK's built in word lemmatizer\n",
    "# In general, it helps to tag the part of speech before lemmatizing\n",
    "# Tagging all words example\n",
    "wordsPOS = nltk.pos_tag(words, tagset='universal') # universal tagset\n",
    "print(wordsPOS)\n",
    "print()\n",
    "print(\"Output type: \", type(wordsPOS), type(wordsPOS[0]))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9668ce5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('goal', 'NOUN'), ('course', 'NOUN'), ('provide', 'VERB'), ('advanced', 'ADJ'), ('training', 'NOUN'), ('data', 'NOUN'), ('science', 'NOUN'), ('encompassing', 'VERB'), ('current', 'ADJ'), ('theory', 'NOUN'), ('application', 'NOUN'), ('real', 'ADJ'), ('world', 'NOUN'), ('data', 'NOUN'), ('analytics', 'NOUN'), ('problems', 'NOUN'), ('course', 'NOUN'), ('builds', 'VERB'), ('fundamentals', 'NOUN'), ('established', 'VERB'), ('csci', 'ADJ'), ('3360', 'NUM'), ('data', 'NOUN'), ('science', 'NOUN'), ('exploring', 'VERB'), ('advanced', 'ADJ'), ('topics', 'NOUN'), ('nonlinear', 'ADJ'), ('dimensionality', 'NOUN'), ('reduction', 'NOUN'), ('semi', 'NOUN'), ('supervised', 'VERB'), ('learning', 'VERB'), ('graph', 'NOUN'), ('algorithms', 'ADV'), ('deep', 'ADJ'), ('learning', 'NOUN')]\n",
      "\n",
      "Output type:  <class 'list'> <class 'tuple'>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tagging after removing stop words\n",
    "wordsPOS = nltk.pos_tag(wordsRelevant, tagset='universal') # universal tagset\n",
    "print(wordsPOS)\n",
    "print()\n",
    "print(\"Output type: \", type(wordsPOS), type(wordsPOS[0]))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcc4d34",
   "metadata": {},
   "source": [
    "#### Notice how in the output above some of the words are clearly mislabeled\n",
    "* Algorithms is not an adverb in this case\n",
    "    * This has happened because the tagger uses sentence structure\n",
    "    * This will affect the lemmatizer output in most cases\n",
    "    * We have to tag the text before removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74c6496a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('They', 'PRON'),\n",
       " ('refuse', 'VERB'),\n",
       " ('to', 'PRT'),\n",
       " ('permit', 'VERB'),\n",
       " ('us', 'PRON'),\n",
       " ('to', 'PRT'),\n",
       " ('obtain', 'VERB'),\n",
       " ('the', 'DET'),\n",
       " ('refuse', 'NOUN'),\n",
       " ('permit', 'NOUN')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another quick example from the NLTK Book\n",
    "# Take notice of how repeated words have different parts of speech\n",
    "# There are many different tagsets you can utilize\n",
    "text = word_tokenize(\"They refuse to permit us to obtain the refuse permit\")\n",
    "nltk.pos_tag(text, tagset='universal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eda7f03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('goal', 'NOUN'), ('course', 'NOUN'), ('provide', 'VERB'), ('advanced', 'ADJ'), ('training', 'NOUN'), ('data', 'NOUN'), ('science', 'NOUN'), ('encompassing', 'VERB'), ('current', 'ADJ'), ('theory', 'NOUN'), ('application', 'NOUN'), ('real', 'ADJ'), ('world', 'NOUN'), ('data', 'NOUN'), ('analytics', 'NOUN'), ('problems', 'NOUN'), ('course', 'NOUN'), ('builds', 'VERB'), ('fundamentals', 'NOUN'), ('established', 'VERB'), ('csci', 'NOUN'), ('3360', 'NUM'), ('data', 'NOUN'), ('science', 'NOUN'), ('exploring', 'VERB'), ('advanced', 'ADJ'), ('topics', 'NOUN'), ('nonlinear', 'ADJ'), ('dimensionality', 'NOUN'), ('reduction', 'NOUN'), ('semi', 'NOUN'), ('supervised', 'VERB'), ('learning', 'VERB'), ('graph', 'NOUN'), ('algorithms', 'NOUN'), ('deep', 'ADJ'), ('learning', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "# Here we tag the pos of each word and then remove any instances of irrelevant words\n",
    "wordsPOS = nltk.pos_tag(words, tagset='universal') # universal tagset\n",
    "cleanedList = []\n",
    "for word in wordsPOS:\n",
    "    if word[0].lower() not in stpWords:\n",
    "        cleanedList.append((word[0].lower(), word[1]))\n",
    "print(cleanedList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea53f156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('goal', 'n'), ('course', 'n'), ('provide', 'v'), ('advanced', 'a'), ('training', 'n'), ('data', 'n'), ('science', 'n'), ('encompassing', 'v'), ('current', 'a'), ('theory', 'n'), ('application', 'n'), ('real', 'a'), ('world', 'n'), ('data', 'n'), ('analytics', 'n'), ('problems', 'n'), ('course', 'n'), ('builds', 'v'), ('fundamentals', 'n'), ('established', 'v'), ('csci', 'n'), ('3360', 'n'), ('data', 'n'), ('science', 'n'), ('exploring', 'v'), ('advanced', 'a'), ('topics', 'n'), ('nonlinear', 'a'), ('dimensionality', 'n'), ('reduction', 'n'), ('semi', 'n'), ('supervised', 'v'), ('learning', 'v'), ('graph', 'n'), ('algorithms', 'n'), ('deep', 'a'), ('learning', 'n')]\n"
     ]
    }
   ],
   "source": [
    "# These pos tags are useful, but must be converted to acceptable lemmatizer parameters.\n",
    "# We can define a method to do this for us.\n",
    "# Please note this is a simple example and does not account for the more indepth word tags\n",
    "def pos_for_lem(wordsPOS):\n",
    "    newTags = []\n",
    "    for tup in wordsPOS:\n",
    "        converted = tup[1][0].lower() # Takes the first letter of the lowercase tag\n",
    "        newTup = (tup[0], converted)\n",
    "        newTags.append(newTup)\n",
    "    return newTags\n",
    "print(pos_for_lem(cleanedList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56597660",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "goal course provide advanced training data science encompass current theory application real world data analytics problem course build fundamental establish csci 3360 data science explore advanced topic nonlinear dimensionality reduction semi supervise learn graph algorithm deep learning\n"
     ]
    }
   ],
   "source": [
    "# We use these tags to parameterize a lemmatizer\n",
    "# The wordnet lemmatizer is provided in the library\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemm = WordNetLemmatizer()\n",
    "# We convert the tags using our predefined method\n",
    "wordsPOS_LEM = pos_for_lem(cleanedList)\n",
    "wordsLemm = [lemm.lemmatize(word, pos) for word, pos in wordsPOS_LEM]\n",
    "#wordsLemm = [lemm.lemmatize(word) for word in words]\n",
    "print()\n",
    "print(toString(wordsLemm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9b87458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goal course provide advanced training data science encompassing current theory application real world data analytics problems course builds fundamentals established csci 3360 data science exploring advanced topics nonlinear dimensionality reduction semi supervised learning graph algorithms deep learning\n"
     ]
    }
   ],
   "source": [
    "# We can compare this output with the original input\n",
    "print(toString(wordsRelevant))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ef105f",
   "metadata": {},
   "source": [
    "##### While the effects on a small string example appear inconsequential, this can reduce a great amount of variability in large bodies of text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
